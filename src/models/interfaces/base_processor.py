import logging
import os
import sys
import time
import queue
import threading
from logging.handlers import QueueHandler, QueueListener
from pathlib import Path
from typing import List, Tuple, Iterable, runtime_checkable, Protocol, TypeVar, Generic
from concurrent.futures import ThreadPoolExecutor
from collections import defaultdict

from pydantic import ValidationError, BaseModel


# çº¿ç¨‹å®‰å…¨çš„æ—¥å¿—ç³»ç»Ÿ
class LogSystem:
    _instance = None
    _lock = threading.Lock()

    def __new__(cls):
        with cls._lock:
            if not cls._instance:
                cls._instance = super().__new__(cls)
                cls._setup()
        return cls._instance

    @classmethod
    def _setup(cls):
        """çº¿ç¨‹å®‰å…¨çš„æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–"""
        cls.log_queue = queue.Queue(-1)  # æ— ç•Œé˜Ÿåˆ—

        # æ—¥å¿—å¤„ç†å™¨é…ç½®
        file_handler = logging.FileHandler("watermark.log")
        stream_handler = logging.StreamHandler()
        # å¢žå¼ºæ—¥å¿—æ ¼å¼ï¼ˆå¢žåŠ æ¯«ç§’ç²¾åº¦ï¼‰
        formatter = logging.Formatter(
            "%(asctime)s.%(msecs)03d - %(threadName)-18s - [%(levelname)s] - %(message)s",
            datefmt="%Y-%m-%d %H:%M:%S"
        )
        file_handler.setFormatter(formatter)
        stream_handler.setFormatter(formatter)

        # å¯åŠ¨åŽå°æ—¥å¿—ç›‘å¬çº¿ç¨‹
        cls.listener = QueueListener(
            cls.log_queue,
            file_handler,
            stream_handler,
            respect_handler_level=True
        )
        cls.listener.start()
        # cls.listener_thread = threading.Thread(target=cls.listener.start)

    # def start(self):
    #     self.listener_thread.start()

    def shutdown(self):
        """å®‰å…¨å…³é—­æ—¥å¿—ç³»ç»Ÿ"""
        self.listener.stop()
        # self.listener_thread.join()  # ç­‰å¾…ç›‘å¬çº¿ç¨‹å¤„ç†å®Œæˆå¹¶ç»ˆæ­¢
        # # å¼ºåˆ¶æ¸…ç©ºé˜Ÿåˆ—ï¼ˆå¯é€‰ï¼‰
        # while not self.log_queue.empty():
        #     try:
        #         self.log_queue.get_nowait()
        #     except queue.Empty:
        #         break

def timing_decorator(func):
    def wrapper(*args, **kwargs):
        start = time.perf_counter()
        result = func(*args, **kwargs)
        duration = time.perf_counter() - start
        return (duration, result)
    return wrapper

def _default_stats():
    return {'count':0, 'total':0.0}

class ProcessorParams(BaseModel):
    """å‚æ•°åŸºç±»ï¼ˆå®šä¹‰å…¬å…±å­—æ®µï¼‰"""
    opacity: float = 0.8
    output_dir: Path

# æ³›åž‹å‚æ•°çº¦æŸ
T = TypeVar("T", bound=ProcessorParams)

class BaseWatermarkProcessor(Generic[T]):
    """ä¼˜åŒ–åŽçš„å¤šçº¿ç¨‹æ°´å°å¤„ç†å™¨ï¼ˆæ—¥å¿—å¢žå¼ºç‰ˆï¼‰"""

    _SUPPORTED_EXT = {'.jpg', '.jpeg', '.png'}

    def __init__(self, config):
        self._config = config
        self._timings = defaultdict(float)
        self._task_stats = defaultdict(_default_stats)
        self._log_system = LogSystem()
        self._log_queue = self._log_system.log_queue
        self._init_logger()
        self.default_params = self._parse_config(config)

    def _init_logger(self):
        """å¢žå¼ºæ—¥å¿—åˆå§‹åŒ–"""
        self._logger = logging.getLogger(f"{self.__class__.__name__}.{id(self)}")
        self._logger.addHandler(QueueHandler(self._log_queue))
        self._logger.setLevel(logging.INFO)
        self._logger.propagate = False  # é¿å…é‡å¤è®°å½•

    def _print_stats(self):
        """æ‰“å°è¯¦ç»†çš„è€—æ—¶ç»Ÿè®¡"""
        print("\n======== æ€§èƒ½åˆ†æžæŠ¥å‘Š ========")
        print(f"[çº¿ç¨‹æ± åˆå§‹åŒ–] {self._timings['pool_init']:.2f}s")
        print(f"[ä»»åŠ¡åˆ†å‘] {self._timings['task_distribute']:.2f}s")
        print(f"[ç»“æžœæ”¶é›†] {self._timings['result_collect']:.2f}s")
        print(f"[æ€»è€—æ—¶] {self._timings['total']:.2f}s\n")

        print("=== ä»»åŠ¡å¤„ç†ç»Ÿè®¡ ===")
        for task_type, stat in self._task_stats.items():
            avg = stat['total'] / stat['count'] if stat['count'] else 0
            print(f"{task_type}: å¹³å‡{avg:.2f}s | æ€»æ•°{stat['total']:.2f}s | æ¬¡æ•°{stat['count']}")

    def process_batch(self, input_dir: Path, output_dir: Path, **kwargs) -> List[Path]:
        try:
            final_params = self._validate_params(
                params = ProcessorParams(
                    **{**self.default_params, **kwargs},
                    output_dir=output_dir
                )
            )
        except ValidationError as e:
            self.logger.exception(e)
            raise ValueError(f"å‚æ•°æ ¡éªŒå¤±è´¥: {e.errors()}")
        """æ·»åŠ æ‰¹å¤„ç†å„é˜¶æ®µæ—¥å¿—"""
        self._logger.info(f"å¼€å§‹æ‰¹å¤„ç†ä»»åŠ¡ | è¾“å…¥ç›®å½•: {input_dir} | è¾“å‡ºç›®å½•: {output_dir}")
        output_dir.mkdir(parents=True, exist_ok=True)
        tasks = list(self._generate_tasks(input_dir, output_dir))
        if not tasks:
            self._logger.warning("æœªå‘çŽ°å¯å¤„ç†æ–‡ä»¶")
            return []

        try:
            # ç”Ÿæˆä»»åŠ¡é˜¶æ®µæ—¥å¿—
            task_start = time.perf_counter()
            tasks = list(self._generate_tasks(input_dir, output_dir))
            gen_time = time.perf_counter() - task_start
            self._logger.info(
                f"æ‰«æåˆ° {len(tasks)} ä¸ªå¾…å¤„ç†æ–‡ä»¶ | "
                f"è€—æ—¶: {gen_time:.2f}s | "
                f"è·³è¿‡æ–‡ä»¶: {self._scan_skipped} ä¸ª"
            )
            # çº¿ç¨‹æ± é…ç½®æ—¥å¿—
            max_workers = min(os.cpu_count() or 4, len(tasks))
            self._logger.info(
                f"ðŸ› åˆå§‹åŒ–çº¿ç¨‹æ±  | æœ€å¤§å·¥ä½œçº¿ç¨‹: {max_workers} | "
                f"æ€»ä»»åŠ¡æ•°: {len(tasks)} | "
                f"é¢„è®¡å¹¶å‘åº¦: {min(max_workers, len(tasks))}"
            )
            # ä½¿ç”¨çº¿ç¨‹æ± æ›¿ä»£è¿›ç¨‹æ± 
            with ThreadPoolExecutor(
                max_workers=min(os.cpu_count() or 4, len(tasks)),
                initializer=self._init_worker
            ) as executor:
                # è®¡æ—¶å¼€å§‹
                self._timings['pool_init'] = time.perf_counter() - task_start

                # ä»»åŠ¡åˆ†å‘
                task_start = time.perf_counter()
                futures = {
                    executor.submit(self._process_wrapper, task, final_params): task
                    for task in tasks
                }
                self._timings['task_distribute'] = time.perf_counter() - task_start

                # ç»“æžœæ”¶é›†
                collect_start = time.perf_counter()
                results = []
                for future in futures:
                    try:
                        success, output_path = future.result()
                        if success:
                            results.append(output_path)
                    except Exception as e:
                        self._logger.error(f"ä»»åŠ¡å¤±è´¥: {e}", exc_info=True)
                self._timings['result_collect'] = time.perf_counter() - collect_start

                return results
        finally:
            # æ·»åŠ ä»»åŠ¡æ€»ç»“æ—¥å¿—
            success_rate = len(results) / len(tasks) if tasks else 0
            self._logger.info(
                f"ä»»åŠ¡å®Œæˆæ€»ç»“ | æˆåŠŸçŽ‡: {success_rate:.1%} | "
                f"æˆåŠŸ: {len(results)} | å¤±è´¥: {len(tasks) - len(results)}"
            )
            self._timings['total'] = time.perf_counter() - task_start
            self._print_stats()

    def _generate_tasks(self, input_dir: Path, output_dir: Path) -> Iterable[Tuple[Path, Path]]:
        """æ·»åŠ ä»»åŠ¡ç”Ÿæˆæ—¥å¿—"""
        self._scan_skipped = 0
        for entry in os.scandir(input_dir):
            src_path = Path(entry.path)
            if entry.is_file() and src_path.suffix.lower() in self._SUPPORTED_EXT:
                dest_path = output_dir / entry.name
                self._logger.debug(f"æ·»åŠ å¤„ç†ä»»åŠ¡: {src_path} â†’ {dest_path}")
                yield (src_path, dest_path)
            else:
                self._scan_skipped += 1
                self._logger.debug(f"è·³è¿‡éžæ”¯æŒæ–‡ä»¶: {src_path}")

    @staticmethod
    def _init_worker():
        """å¢žå¼ºå·¥ä½œçº¿ç¨‹æ—¥å¿—"""
        thread_id = threading.get_ident()
        logger = logging.getLogger()
        logger.info(f"å·¥ä½œçº¿ç¨‹å¯åŠ¨ | TID: {thread_id} | å‡†å¤‡å°±ç»ª")

    def _process_wrapper(self, task: Tuple[Path, Path], kwargs: ProcessorParams) -> Tuple[bool, Path]:
        """æ·»åŠ è¯¦ç»†ä»»åŠ¡æ—¥å¿—"""
        input_path, output_path = task
        thread_name = threading.current_thread().name
        try:
            # ä»»åŠ¡å¼€å§‹æ—¥å¿—
            self._logger.info(
                f"å¼€å§‹å¤„ç†æ–‡ä»¶ | çº¿ç¨‹: {thread_name} | "
                f"è¾“å…¥: {input_path} | è¾“å‡º: {output_path}"
            )
            start_time = time.perf_counter()
            self.process_single(input_path, output_path, kwargs)
            cost = time.perf_counter() - start_time
            self._task_stats['process_single']['count'] += 1
            self._task_stats['process_single']['total'] += cost
            # æˆåŠŸæ—¥å¿—
            self._logger.info(
                f"å¤„ç†æˆåŠŸ | çº¿ç¨‹: {thread_name} | "
                f"è€—æ—¶: {cost:.2f}s | è¾“å‡ºæ–‡ä»¶: {output_path}"
            )
            return (True, output_path)
        except Exception as e:
            # å¤±è´¥æ—¥å¿—ï¼ˆåŒ…å«å¼‚å¸¸ç±»åž‹ï¼‰
            error_type = type(e).__name__
            self._logger.error(
                f"å¤„ç†å¤±è´¥ | çº¿ç¨‹: {thread_name} | "
                f"æ–‡ä»¶: {input_path} | é”™è¯¯ç±»åž‹: {error_type} | è¯¦æƒ…: {str(e)}",
                exc_info=True
            )
            return (False, output_path)

    def process_single(
        self,
        input_path: Path,
        output_path: Path,
        params: T  # æ³›åž‹å‚æ•°
    ) -> None:
        """å…·ä½“å¤„ç†é€»è¾‘ï¼ˆéœ€å­ç±»å®žçŽ°ï¼‰"""
        self._validate_params(params)
        raise NotImplementedError

    def _validate_params(self, params: T):
        """è¿”å›žå…·ä½“å‚æ•°ç±»åž‹ï¼ˆå­ç±»å®žçŽ°ï¼‰"""
        raise NotImplementedError

    @property
    def log_system(self) -> LogSystem:
        return self._log_system

    @property
    def logger(self) -> logging.Logger:
        return self._logger

    @property
    def config(self):
        return self._config

    def _parse_config(self, config):
        return {**config}
